{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "709cc878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import tqdm\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from Cleaning import data_preprocess\n",
    "import pickle\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "# Getting simillar words using glove\n",
    "import os\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import spatial\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "stop_words=set(stopwords.words('English'))\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "stop_words=set(stopwords.words('English'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7818a594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # global emmbed_dict, quant_list\n",
    "# ## List of mesurable quantities and its units\n",
    "# quant_list = ['length', 'area', 'volume', 'weight', 'data', 'speed', 'acres', 'ares', 'hectares', 'square', 'feet', 'fts', 'ft', 'inchs', 'inch', 'inches', 'yards', 'yard', 'yd', 'miles', 'mile', 'mils', 'fahrenheit', 'gallons', 'gallon', 'litres', 'millilitres', 'cubic', 'tons', 'ton', 'pounds', 'bit', 'bits', 'byte', 'bytes', 'kilobyte', 'kilobytes', 'megabyte', 'megabytes', 'gigabyte', 'gigabytes', 'terabyte', 'terabytes', 'per', 'mass', 'time', 'temperature', 'electric', 'current', 'second', 'seconds', 'sec', ' minute', 'min', 'minutes', 'hour', 'hours', 'hrs', 'day', 'days', 'week', 'weeks', 'year', 'years', 'decade', 'decades', 'century', 'centuries', 'millimeters', 'mm', 'millimetre', 'centimeters', 'cm', 'meters,m', 'mtrs', 'metre and kilometers', 'km', 'kilogram', 'kg', 'ounce', 'oz', 'pound', 'lbs', 'gram', 'gm', 'degrees', 'celsius', 'C', 'kelvin', 'K']\n",
    "# emmbed_dict = {}\n",
    "# with open('/Users/sanikakatekar/Downloads/Jupyter Notebooks/glove.6B.200d.txt','r') as f:\n",
    "#     for line in f:\n",
    "#         values = line.split()\n",
    "#         word = values[0]\n",
    "#         vector = np.asarray(values[1:],'float32')\n",
    "#         emmbed_dict[word]=vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3af3104c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(quant_list, open(\"measurable_quantities.pkl\", \"wb\"))\n",
    "# pickle.dump(emmbed_dict, open(\"glove200d.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d8ad4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_list = pickle.load(open(\"measurable_quantities.pkl\", \"rb\"))\n",
    "emmbed_dict = pickle.load(open(\"glove200d.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12bce79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_word(main_word, topn=50):\n",
    "    nearest = sorted(emmbed_dict.keys(), key=lambda word: spatial.distance.cosine(emmbed_dict[word], main_word))\n",
    "    return nearest[1:topn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75f04d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_synonym(bigram, sen_pos):\n",
    "    original_word, main_tag = sen_pos[bigram[0]]\n",
    "    syn = original_word\n",
    "#     if (main_tag in ['NN','CD','RB','MD','VBN','VBD','NNP','NNPS']) or:\n",
    "#         return original_word\n",
    "    score = 100\n",
    "    original_word = original_word.lower()\n",
    "    context_word_1 = sen_pos[bigram[1]][0].lower()\n",
    "    context_word_2 = sen_pos[bigram[2]][0].lower()\n",
    "    vec_org_wrd = emmbed_dict[original_word]\n",
    "    vec_context = emmbed_dict[context_word_1] + emmbed_dict[context_word_2]\n",
    "\n",
    "#     def get_lowest(wrd, score):\n",
    "#         vec_1 = vec_org_wrd + vec_context\n",
    "#         vec_2 = emmbed_dict[wrd] + vec_context\n",
    "#         bigram_cosine = spatial.distance.cosine(vec_1, vec_2)\n",
    "#         if bigram_cosine < score:\n",
    "#             score = bigram_cosine\n",
    "#             syn = wrd\n",
    "#         return syn, score\n",
    "    \n",
    "    for wrd in find_similar_word(vec_org_wrd, topn=50):\n",
    "        syn_tag = nltk.tag.pos_tag([wrd])[0][1]\n",
    "        if main_tag == syn_tag:\n",
    "            vec_1 = vec_org_wrd + vec_context\n",
    "            vec_2 = emmbed_dict[wrd] + vec_context\n",
    "            bigram_cosine = spatial.distance.cosine(vec_1, vec_2)\n",
    "            if bigram_cosine < score:\n",
    "                score = bigram_cosine\n",
    "                syn = wrd\n",
    "#             syn, score = get_lowest(wrd, score)\n",
    "        elif (main_tag == 'CC') and (syn_tag == 'IN'):\n",
    "            vec_1 = vec_org_wrd + vec_context\n",
    "            vec_2 = emmbed_dict[wrd] + vec_context\n",
    "            bigram_cosine = spatial.distance.cosine(vec_1, vec_2)\n",
    "            if bigram_cosine < score:\n",
    "                score = bigram_cosine\n",
    "                syn = wrd\n",
    "#             syn, score = get_lowest(wrd, score)\n",
    "    return syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19ae8a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rephrase_sen(sens):\n",
    "    summary = []\n",
    "    for sen in tqdm_notebook(sens):\n",
    "#     for sen in sens:\n",
    "        new_sen = \"\"\n",
    "        ## Basic cleaning\n",
    "        original_articles, cleaned_articles, cleaned_articles_merged = data_preprocess([sen])\n",
    "        cleaned_articles_merged  = cleaned_articles_merged[-1]\n",
    "        cleaned_articles = cleaned_articles[-1]\n",
    "        original_articles = original_articles[-1]\n",
    "        \n",
    "        words = word_tokenize(sen)\n",
    "        sen_pos = nltk.tag.pos_tag(words)\n",
    "        \n",
    "        sen_entity = [X.text for X in nlp(sen).ents] + quant_list\n",
    "#         for wrd in tqdm_notebook(words):\n",
    "        for wrd in words:\n",
    "            if (wrd in sen_entity) or (wrd.lower() in stop_words):\n",
    "                new_sen += wrd + \" \"\n",
    "            elif wrd.isalpha():\n",
    "                for idx, pos in enumerate(sen_pos):\n",
    "                    word, tag = pos\n",
    "                    if (word == wrd) and (tag not in ['NN','CD','RB','MD','VBN','VBD','NNP','NNPS']):\n",
    "                        if idx == 0:\n",
    "                            ids = (idx, idx+1, idx+2)\n",
    "                        elif ((idx+1)==len(sen_pos)):\n",
    "                            ids = (idx, idx-1, idx-2)\n",
    "                        else:\n",
    "                            ids = (idx, idx-1, idx+1)\n",
    "                        synonym = get_best_synonym(ids, sen_pos)\n",
    "                        new_sen += synonym + \" \"\n",
    "                    elif (word == wrd):\n",
    "                        new_sen += wrd + \" \"    \n",
    "            else:\n",
    "                new_sen += wrd + \" \"\n",
    "        summary.append(new_sen)\n",
    "    summary = [sen.strip() + \".\" for sen in summary]\n",
    "    return \" \".join(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "174addea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted_sen = [\"But at the time these images were taken, Hitler's Berlin was vibrant\", \n",
    "#                  \"Just two years later Germany would invade Poland and begin the most destructive war the world has ever seen\", \n",
    "#                  \"An estimated 60 million people lost their lives as a result of the Second World War and the global political landscape changed forever\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "205b7f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "455afb350e19429d8ef25bbd18dacb13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"But at the time these pictures were taken , Hitler 's Berlin was prosperous. Just two years later Germany would destabilize Poland and take the most dangerous war the world has ever seen. An estimated 60 million others lost their people as a result of the Second World War and the financial social landscape changed forever.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rephrase_sen(extracted_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4741bf1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
