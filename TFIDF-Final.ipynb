{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d29eda82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import nltk\n",
    "import pandas as pd \n",
    "from operator import itemgetter\n",
    "from nltk import sent_tokenize, word_tokenize, PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from Cleaning import data_preprocess\n",
    "\n",
    "count_vect_model = pickle.load(open(r\"Data/count_vect_model.pkl\", \"rb\"))\n",
    "tfidf_model = pickle.load(open(r\"Data/tf_idf_model.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357928eb",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3fca2c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_train = pd.read_csv(r\"Data/cleaned_training_data.csv\")\n",
    "# original_articles, cleaned_articles, cleaned_articles_merged = pickle.load(open(r\"Data/cleaned_training_data.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f08f9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function definition to Train TF-IDF Model\n",
    "# def tf_idf_train(cleaned_articles_merged):\n",
    "#     count_vect = CountVectorizer()\n",
    "#     tfidf = TfidfTransformer(norm=\"l2\")\n",
    "#     count_vect_model = count_vect.fit(cleaned_articles_merged)\n",
    "#     freq_term_matrix = count_vect_model.transform(cleaned_articles_merged)\n",
    "#     tf_idf_model= tfidf.fit(freq_term_matrix)\n",
    "#     return count_vect_model, tf_idf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea64dc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training,Saving the Models\n",
    "# count_vect_model, tf_idf_model =tf_idf_train(cleaned_articles_merged) # Training\n",
    "# pickle.dump(count_vect_model, open(r\"C:\\Users\\PRAVEEN\\Desktop\\MSAI\\1. University Of Georgia, Athens\\UGA MSAI Program Material\\3. Natural Language Processing\\Project\\Code and Filtered Data\\count_vect_model.pkl\", \"wb\")) #Store count_vec model\n",
    "# pickle.dump(tf_idf_model, open(r\"C:\\Users\\PRAVEEN\\Desktop\\MSAI\\1. University Of Georgia, Athens\\UGA MSAI Program Material\\3. Natural Language Processing\\Project\\Code and Filtered Data\\tf_idf_model.pkl\", \"wb\")) #Store TFIDF model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dd6309",
   "metadata": {},
   "source": [
    "### Testing the Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ebaf1397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rank_sentences(test_article_str, test_article_matrix, feature_names, top_n=3):\n",
    "#     sents = nltk.sent_tokenize(test_article_str)\n",
    "#     sentences = [nltk.word_tokenize(sent) for sent in sents]\n",
    "#     tfidf_sent = [[test_article_matrix[feature_names.index(w.lower())]\n",
    "#                    for w in sent if w.lower() in feature_names]\n",
    "#                  for sent in sentences]\n",
    "#     doc_val = sum(test_article_matrix)\n",
    "#     sent_values = [sum(sent) / doc_val for sent in tfidf_sent]\n",
    "#     ranked_sents = [pair for pair in zip(range(len(sent_values)), sent_values)]\n",
    "#     ranked_sents = sorted(ranked_sents, key=lambda x: x[1] *-1)\n",
    "#     return ranked_sents[:top_n]\n",
    "\n",
    "# def tf_idf_summarizer(test_article_list, original_articles, topn): # or take one argument and clean inside the function\n",
    "#     # article_merged\n",
    "#     test_article_str = \". \".join(test_article_list) # convert the text from list of strings to a single piece of text\n",
    "#     #arti(input)\n",
    "#     original_articles_str = \". \".join(original_articles)\n",
    "    \n",
    "#     # Cleaning\n",
    "# #     original_articles, cleaned_articles, cleaned_articles_merged = data_preprocess([article])\n",
    "# #     cleaned_articles = cleaned_articles[-1]\n",
    "# #     original_articles = original_articles[-1]\n",
    "    \n",
    "#     feature_names = count_vect_model.get_feature_names()\n",
    "# # Get the dense tf-idf matrix for the document\n",
    "#     test_article_term_matrix  = count_vect_model.transform(test_article_list) #cleaned\n",
    "#     test_article_tfidf_matrix = tfidf.transform(test_article_term_matrix)\n",
    "#     test_article_dense  = test_article_tfidf_matrix.todense()\n",
    "#     test_article_matrix = test_article_dense.tolist()[0]\n",
    "# #Writing summary\n",
    "#     top_sents = rank_sentences(test_article_str, test_article_matrix, feature_names,top_n=n)\n",
    "#     top_sents=sorted(top_sents, key=itemgetter(0))\n",
    "#     summary = '.'.join([original_articles_str.split('.')[i]\n",
    "#                     for i in [pair[0] for pair in top_sents]])\n",
    "#     summary = ' '.join(summary.split())\n",
    "#     return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "32689c78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tf_idf_summarizer(cleaned_articles[24998], original_articles[24998], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac44f74c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6de5f72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_sentences(test_article_str, test_article_matrix, feature_names, top_n=3):\n",
    "    sents = nltk.sent_tokenize(test_article_str)\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sents]\n",
    "    tfidf_sent = [[test_article_matrix[feature_names.index(w.lower())]\n",
    "                   for w in sent if w.lower() in feature_names]\n",
    "                 for sent in sentences]\n",
    "    doc_val = sum(test_article_matrix)\n",
    "    sent_values = [sum(sent) / doc_val for sent in tfidf_sent]\n",
    "    ranked_sents = [pair for pair in zip(range(len(sent_values)), sent_values)]\n",
    "    ranked_sents = sorted(ranked_sents, key=lambda x: x[1] *-1)\n",
    "    return ranked_sents[:top_n]\n",
    "\n",
    "def tfidf_transform(article, topn=5): # or take one argument and clean inside the function\n",
    "    # Cleaning\n",
    "    original_articles, cleaned_articles, cleaned_articles_merged = data_preprocess([article])\n",
    "    test_article_str = \". \".join(cleaned_articles[-1])\n",
    "    original_articles_str = \". \".join(original_articles[-1])\n",
    "    \n",
    "    feature_names = count_vect_model.get_feature_names()\n",
    "# Get the dense tf-idf matrix for the document\n",
    "    test_article_term_matrix  = count_vect_model.transform(cleaned_articles[-1]) #cleaned\n",
    "    test_article_tfidf_matrix = tfidf_model.transform(test_article_term_matrix)\n",
    "    test_article_dense  = test_article_tfidf_matrix.todense()\n",
    "    test_article_matrix = test_article_dense.tolist()[0]\n",
    "#Writing summary\n",
    "    top_sents = rank_sentences(test_article_str, test_article_matrix, feature_names, top_n = topn)\n",
    "    top_sents=sorted(top_sents, key=itemgetter(0))\n",
    "    summary = [original_articles[-1][i[0]] for i in top_sents]\n",
    "#     summary = '.'.join([original_articles_str.split('.')[i] for i in [pair[0] for pair in top_sents]])\n",
    "#     summary = ' '.join(summary.split())\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc59e72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6f16e369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A schoolboy was airlifted to hospital after being hit by a piece of adventure playground equipment that collapsed on top of him', 'The playground where the incident happened is thought to be part of this installation at the Cherry Lane Adventure Playground', 'A number of police cars and paramedics attended the scene at the Cherry Lane Adventure Playground', 'In addition, the police cordoned off the adventure playground with police tape, so that other children could not use it', \"The boy was playing on the adventure playground's installations when a piece of the park, a pole, collapsed and struck him on the head\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"A schoolboy was airlifted to hospital after being hit by a piece of adventure playground equipment that collapsed on top of him. The playground where the incident happened is thought to be part of this installation at the Cherry Lane Adventure Playground. A number of police cars and paramedics attended the scene at the Cherry Lane Adventure Playground. In addition, the police cordoned off the adventure playground with police tape, so that other children could not use it. The boy was playing on the adventure playground's installations when a piece of the park, a pole, collapsed and struck him on the head\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tfidf_transform(data_train['article'][24998])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3235bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f3ce7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
