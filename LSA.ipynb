{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b880d752",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libraries\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "en = spacy.load('en_core_web_sm')\n",
    "sw_spacy = en.Defaults.stop_words\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "# Gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import re\n",
    "from gensim.models import TfidfModel, LsiModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim import matutils\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load models\n",
    "lsa_model = pickle.load(open(r\"Data\\\\lsa_model_6.pkl\", \"rb\"))\n",
    "dictionary = pickle.load(open(r\"Data\\\\dictionary.pkl\", \"rb\"))\n",
    "corpus = pickle.load(open(r\"Data\\\\corpus.pkl\", \"rb\"))\n",
    "topic_dict = pickle.load(open(r\"Data\\\\topic_dict_lsa.pkl\", \"rb\"))\n",
    "texts = pickle.load(open(r\"Data\\\\texts.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c40ed8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load stored data\n",
    "# data_train = pd.read_csv(r\"Data\\\\cleaned_training_data.csv\")\n",
    "# original_articles, cleaned_articles, cleaned_articles_merged = pickle.load(open(r\"Data\\\\cleaned_training_data.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4c9eb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 5000\n",
    "# data_train = data_train[:n]\n",
    "# original_articles = original_articles[:n]\n",
    "# cleaned_articles = cleaned_articles[:n]\n",
    "# cleaned_articles_merged = cleaned_articles_merged[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2c2f39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_train: 25000\n",
      "original_articles: 25000\n",
      "cleaned_articles: 25000\n",
      "cleaned_articles_merged: 25000\n"
     ]
    }
   ],
   "source": [
    "# print(\"data_train:\",len(data_train))\n",
    "# print(\"original_articles:\",len(original_articles))\n",
    "# print(\"cleaned_articles:\",len(cleaned_articles))\n",
    "# print(\"cleaned_articles_merged:\",len(cleaned_articles_merged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e48b8d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts = [[word for word in article.split(\" \")] for article in cleaned_articles_merged[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4ec2f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary = corpora.Dictionary(texts)\n",
    "# # print(dictionary.id2token) ## to see the actual dictionary generated\n",
    "# corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89121747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fit LSA model\n",
    "# lsa_model = LsiModel(corpus=corpus, id2word=dictionary, num_topics=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7275011e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create Topic-Word dictionary\n",
    "# topic_dict = {}\n",
    "# for topic in range(0, lsa_model.num_topics):\n",
    "#     temp = {}\n",
    "#     for token, score in lsa_model.show_topic(topic, topn=len(dictionary)):\n",
    "#         if token.isalpha():\n",
    "#             topic_dict[str(topic)+\"_\"+token] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4c3ee3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Store model\n",
    "# pickle.dump(lsa_model, open(r\"Data\\\\lsa_model_6.pkl\", \"wb\"))\n",
    "# pickle.dump(topic_dict, open(r\"Data\\\\topic_dict_lsa.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed1d7060",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ee498af",
   "metadata": {},
   "source": [
    "#### Sentence ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "802c7733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_weights_lsa(topic_dict, assigned_topic):\n",
    "    weights_dict =  {}\n",
    "    for topic_word, score in topic_dict.items():\n",
    "        topic, word = topic_word.split(\"_\")\n",
    "        if topic == assigned_topic:\n",
    "            weights_dict[word] = score\n",
    "    return weights_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0ec845b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sentences_lsa(weights_dict, original_articles, cleaned_articles):\n",
    "    sen_score = {}\n",
    "    for idx, sen in enumerate(cleaned_articles):\n",
    "        if len(sen) > 3:\n",
    "            score = 0\n",
    "            for word in sen.split(\" \"):\n",
    "                if word in weights_dict:\n",
    "                    score += weights_dict[word]\n",
    "            sen_score[original_articles[idx]] = score\n",
    "    sen_score = sorted(sen_score, key=sen_score.get, reverse=True)\n",
    "    return sen_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d22a21ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsa_transform(article, topn=5):\n",
    "    # Cleaning\n",
    "    original_articles, cleaned_articles, cleaned_articles_merged = pickle.load(open(r\"Data\\\\cleaned_training_data.pkl\", \"rb\"))\n",
    "    cleaned_articles_merged = cleaned_articles_merged[-1]\n",
    "    cleaned_articles = cleaned_articles[-1]\n",
    "    original_articles = original_articles[-1]\n",
    "    \n",
    "    # Load models\n",
    "    lsa_model = pickle.load(open(r\"Data\\\\lsa_model_10.pkl\", \"rb\"))\n",
    "    dictionary = pickle.load(open(r\"Data\\\\dictionary.pkl\", \"rb\"))\n",
    "#     corpus = pickle.load(open(r\"Data\\\\corpus.pkl\", \"rb\"))\n",
    "    topic_dict = pickle.load(open(r\"Data\\\\topic_dict_lsa.pkl\", \"rb\"))\n",
    "    \n",
    "    # Create a new corpus, made of previously unseen documents.\n",
    "    texts_new = [[word for word in article.split(\" \")] for article in cleaned_articles_merged]\n",
    "    other_corpus = [dictionary.doc2bow(text) for text in texts_new]\n",
    "    assigned_topic = max(lsa_model[other_corpus][0], key = lambda i : i[1])[0] \n",
    "    weights_dict = create_new_weights_lsa(topic_dict, assigned_topic)\n",
    "    sen_score = score_sentences_lsa(weights_dict, original_articles, cleaned_articles)\n",
    "    return sen_score[:topn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8837325e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b7e86187aa48058cfdf325b2c19837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['This collection of rare color photos of Berlin in 1937, taken by Thomas Neumann and uncovered from Norwegian archives, show life in the German capital during a tumultuous decade',\n",
       " 'They capture scenes in the vibrant city, which was under the iron grip of Adolf Hitler and his Third Reich at the very height of his power',\n",
       " 'Yet just eight years later the city was in ruins as Russians and Allies occupied it in victory',\n",
       " \"But at the time these images were taken, Hitler's Berlin was vibrant\",\n",
       " \"Hitler had taken power after the collapse of the democratic Weimar Republic in 1933 as severe economic problems caused by the Great Depression drove ordinary Germans into the far-right party's arms\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary = lsa_transform(\"article\", topn=5)\n",
    "# summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d360888b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fada2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc16d89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c09984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca358f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d0e9b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d49620ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a new corpus, made of previously unseen documents.\n",
    "# texts_new = [[word for word in article.split(\" \")] for article in [cleaned_articles_merged[-1]]]\n",
    "# other_corpus = [dictionary.doc2bow(text) for text in texts_new]\n",
    "# # unseen_doc = other_corpus[0]\n",
    "# assigned_topic = max(lsa_model[other_corpus][0], key = lambda i : i[1])[0]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa3ec8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sen_score = {}\n",
    "# weights_dict = topic_dict[\"Topic_\"+str(assigned_topic)]\n",
    "# for idx, sen in enumerate(cleaned_articles[-1]):\n",
    "#     if len(sen) > 3:\n",
    "#         score = 0\n",
    "#         for word in sen.split(\" \"):\n",
    "#             if word in weights_dict:\n",
    "#                 score += weights_dict[word]\n",
    "#         sen_score[original_articles[-1][idx]] = score\n",
    "# sen_score = sorted(sen_score, key=sen_score.get, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b05df82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['An estimated 60 million people lost their lives as a result of the Second World War and the global political landscape changed forever',\n",
       " 'Just two years later Germany would invade Poland and begin the most destructive war the world has ever seen',\n",
       " \"Ripple: The 1937 May Day celebration was also a celebration of 700 years of Berlin's history\",\n",
       " \"Hitler had taken power after the collapse of the democratic Weimar Republic in 1933 as severe economic problems caused by the Great Depression drove ordinary Germans into the far-right party's arms\",\n",
       " 'Eight years later it would have been filled with Russian, British and American troops']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sen_score[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ff0192",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
